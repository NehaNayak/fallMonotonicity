#cd WordNet-3.0/dict/
#cat index.noun | tr ' ' '\t' | cut -f1 |awk 'NF'> ../../nounTokens.txt
#cat noun.exc | tr ' ' '\t' | cut -f1 >> ../../nounTokens.txt 
#cat index.verb | tr ' ' '\t' | cut -f1 |awk 'NF'> ../../verbTokens.txt
#cat verb.exc | tr ' ' '\t' | cut -f1 >> ../../verbTokens.txt 
#cat index.adj | tr ' ' '\t' | cut -f1 |awk 'NF'> ../../adjTokens.txt
#cat adj.exc | tr ' ' '\t' | cut -f1 >> ../../adjTokens.txt 
#cat index.adv | tr ' ' '\t' | cut -f1 |awk 'NF'> ../../advTokens.txt
#cat adv.exc | tr ' ' '\t' | cut -f1 >> ../../advTokens.txt 
#cd ../../

# Back to wordnet directory!
#sort adjTokens.txt | uniq -c | tr -s ' ' '\t' | cut -f3 > temp
#mv temp adjTokens.txt 
#sort nounTokens.txt | uniq -c | tr -s ' ' '\t' | cut -f3 > temp
#mv temp nounTokens.txt 
#sort verbTokens.txt | uniq -c | tr -s ' ' '\t' | cut -f3 > temp
#mv temp verbTokens.txt 
#sort advTokens.txt | uniq -c | tr -s ' ' '\t' | cut -f3 > temp
#mv temp advTokens.txt 

# Make sense index
#python formatWordnet.py synset_definition.txt

# Compute word forms
#python formatWordnet.py

#cat adjTokens_expanded.txt nounTokens_expanded.txt verbTokens_expanded.txt advTokens.txt | sort | uniq > expandedVocab.txt

# Make token - synset mapping
#python formatWordnet.py

# Fake clustering with 31 max senses
cat token_synset.txt | python fakeClustering.py > token_synset_fakeCluster_31.txt
cut -f3 token_synset.txt | sort | uniq > original_synsets.txt
cut -f3 token_synset_fakeCluster_31.txt | sort | uniq > fakeCluster_31_synsets.txt
cat original_synsets.txt fakeCluster_31_synsets.txt | sort | uniq -c | awk '$1==1{print $2}' > fakeCluster_31_missingSynsets.txt 
cat fakeCluster_31_missingSynsets.txt | python whichAreMissing.py > fakeCluster_31_missingSynsets_definitions.txt 
